{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jorgenv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertConfig, BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"bert-base-cased\",\n",
    "        split='train'\n",
    "    ):\n",
    "        self._device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self._config = BertConfig.from_pretrained(model_name)\n",
    "        self._bert_model = BertModel.from_pretrained(model_name, config=self._config)\n",
    "        self._bert_model.eval()\n",
    "        self._bert_tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "        self._data_df = pd.read_csv(f\"../data/{split}_data.csv\", index_col=\"Date\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_df.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._data_df.iloc[index]\n",
    "        label = row[-1]\n",
    "        text_series = row[:-3]\n",
    "        nan_count = text_series.isna().sum()\n",
    "        day_text_matrix = np.empty((text_series.size - nan_count, 768))\n",
    "        for index, text in enumerate(text_series):\n",
    "            if isinstance(text, str):\n",
    "                tokens = self._bert_tokenizer(text, return_tensors='pt')\n",
    "                self._bert_model = self._bert_model.to(self._device)\n",
    "                output = self._bert_model(tokens.input_ids.to(self._device))\n",
    "                latent_matrix = output.last_hidden_state[0]\n",
    "                mean_vector = torch.mean(latent_matrix, 0)\n",
    "                mean_vector = mean_vector.to('cpu').detach().numpy()\n",
    "                mean_vector = mean_vector.reshape((1,-1))\n",
    "                day_text_matrix[index, :] = mean_vector\n",
    "        return (\n",
    "            torch.tensor(day_text_matrix),\n",
    "            torch.tensor(label)\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 768\n",
    "        self.num_layers = 1\n",
    "        self._device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.6,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0, c0 = self.init_hidden(x.size(0))\n",
    "        output, state = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = torch.sigmoid(output)\n",
    "        return output, state\n",
    "\n",
    "    def init_hidden(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self._device),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self._device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(train_dataset, val_dataset, model, device, batch_size=32, max_epochs=100):\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    results = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"train_accuracy\": [],\n",
    "        \"val_accuracy\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        results[\"epoch\"].append(epoch)\n",
    "\n",
    "        train_running_loss = []\n",
    "        train_running_accuracy = []\n",
    "\n",
    "        model = model.train()\n",
    "        for _, (x, y_true) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            y_pred, _ = model(x.float())\n",
    "            y_true = y_true.reshape((-1, 1))\n",
    "            loss = criterion(y_pred, y_true.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss.append(loss.item())\n",
    "\n",
    "            pred = np.round(y_pred.cpu().detach())\n",
    "            target = np.round(y_true.cpu().detach())\n",
    "            accuracy = accuracy_score(target, pred)\n",
    "            train_running_accuracy.append(accuracy)\n",
    "\n",
    "        train_loss = np.mean(train_running_loss)\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        train_accuracy = np.mean(train_running_accuracy)\n",
    "        results[\"train_accuracy\"].append(train_accuracy)\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        val_running_accuracy = []\n",
    "\n",
    "        model = model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for _, (x, y_true) in enumerate(val_dataloader):\n",
    "                x = x.to(device)\n",
    "                y_true = y_true.to(device)\n",
    "                y_pred, _ = model(x.float())\n",
    "                y_true = y_true.reshape((-1, 1))\n",
    "\n",
    "                pred = np.round(y_pred.cpu().detach())\n",
    "                target = np.round(y_true.cpu().detach())\n",
    "                accuracy = accuracy_score(target, pred)\n",
    "                val_running_accuracy.append(accuracy)\n",
    "        \n",
    "        val_accuracy = np.mean(val_running_accuracy)\n",
    "        results[\"val_accuracy\"].append(val_accuracy)\n",
    "        print({ 'epoch': epoch, 'train_loss': train_loss, 'train_accuracy': train_accuracy, 'val_accuracy': val_accuracy })\n",
    "\n",
    "    return results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'train_loss': 0.6958538862791929, 'train_accuracy': 0.5498251748251749, 'val_accuracy': 0.5493055555555555}\n",
      "{'epoch': 1, 'train_loss': 0.6801002364267003, 'train_accuracy': 0.5615166083916084, 'val_accuracy': 0.5524305555555555}\n",
      "{'epoch': 2, 'train_loss': 0.6606309576468035, 'train_accuracy': 0.5984484265734266, 'val_accuracy': 0.5166666666666667}\n",
      "{'epoch': 3, 'train_loss': 0.6584962369366125, 'train_accuracy': 0.6111778846153846, 'val_accuracy': 0.5260416666666667}\n",
      "{'epoch': 4, 'train_loss': 0.6162110600959171, 'train_accuracy': 0.6612215909090909, 'val_accuracy': 0.5010416666666667}\n",
      "{'epoch': 5, 'train_loss': 0.5959938751025633, 'train_accuracy': 0.6789772727272727, 'val_accuracy': 0.5055555555555555}\n",
      "{'epoch': 6, 'train_loss': 0.5686613853004846, 'train_accuracy': 0.7088068181818182, 'val_accuracy': 0.5399305555555556}\n",
      "{'epoch': 7, 'train_loss': 0.5296902781860395, 'train_accuracy': 0.7350852272727273, 'val_accuracy': 0.5243055555555556}\n",
      "{'epoch': 8, 'train_loss': 0.5079379047859799, 'train_accuracy': 0.7521306818181818, 'val_accuracy': 0.5149305555555556}\n",
      "{'epoch': 9, 'train_loss': 0.4863803247836503, 'train_accuracy': 0.7826704545454546, 'val_accuracy': 0.5305555555555556}\n",
      "{'epoch': 10, 'train_loss': 0.433662974698977, 'train_accuracy': 0.8132102272727273, 'val_accuracy': 0.5305555555555556}\n",
      "{'epoch': 11, 'train_loss': 0.3677109129388224, 'train_accuracy': 0.8458806818181818, 'val_accuracy': 0.5274305555555555}\n",
      "{'epoch': 12, 'train_loss': 0.2849424093623053, 'train_accuracy': 0.8941761363636364, 'val_accuracy': 0.5447916666666667}\n",
      "{'epoch': 13, 'train_loss': 0.2774929355152629, 'train_accuracy': 0.8977272727272727, 'val_accuracy': 0.5555555555555556}\n",
      "{'epoch': 14, 'train_loss': 0.26713932102376764, 'train_accuracy': 0.8970170454545454, 'val_accuracy': 0.5305555555555556}\n",
      "{'epoch': 15, 'train_loss': 0.22573125396262517, 'train_accuracy': 0.9098011363636364, 'val_accuracy': 0.5149305555555556}\n",
      "{'epoch': 16, 'train_loss': 0.19423349400643597, 'train_accuracy': 0.9296875, 'val_accuracy': 0.47430555555555554}\n",
      "{'epoch': 17, 'train_loss': 0.13772841754623436, 'train_accuracy': 0.9545454545454546, 'val_accuracy': 0.5260416666666667}\n",
      "{'epoch': 18, 'train_loss': 0.10034292893992229, 'train_accuracy': 0.9765625, 'val_accuracy': 0.5590277777777778}\n",
      "{'epoch': 19, 'train_loss': 0.0829790437340059, 'train_accuracy': 0.9772727272727273, 'val_accuracy': 0.5434027777777778}\n",
      "{'epoch': 20, 'train_loss': 0.05592247454280203, 'train_accuracy': 0.9815340909090909, 'val_accuracy': 0.5274305555555555}\n",
      "{'epoch': 21, 'train_loss': 0.07239490015093576, 'train_accuracy': 0.9772727272727273, 'val_accuracy': 0.4958333333333333}\n",
      "{'epoch': 22, 'train_loss': 0.06408609080509367, 'train_accuracy': 0.9836647727272727, 'val_accuracy': 0.5149305555555556}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m Model(device)\n\u001b[0;32m----> 6\u001b[0m train(train_dataset, val_dataset, model, device)\n",
      "\u001b[1;32m/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb Cell 4'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataset, val_dataset, model, device, batch_size, max_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000005vscode-remote?line=28'>29</a>\u001b[0m train_running_accuracy \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000005vscode-remote?line=30'>31</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000005vscode-remote?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, (x, y_true) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000005vscode-remote?line=32'>33</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000005vscode-remote?line=33'>34</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb Cell 2'\u001b[0m in \u001b[0;36mNewsDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000003vscode-remote?line=32'>33</a>\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bert_tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000003vscode-remote?line=33'>34</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bert_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bert_model\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000003vscode-remote?line=34'>35</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bert_model(tokens\u001b[39m.\u001b[39;49minput_ids\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000003vscode-remote?line=35'>36</a>\u001b[0m latent_matrix \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlast_hidden_state[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/d/Code/UMBC_Capstone_2022/notebooks/modeling.ipynb#ch0000003vscode-remote?line=36'>37</a>\u001b[0m mean_vector \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(latent_matrix, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:995\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=985'>986</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=987'>988</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=992'>993</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=998'>999</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=999'>1000</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1000'>1001</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1001'>1002</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1002'>1003</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1003'>1004</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=572'>573</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=573'>574</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=574'>575</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=578'>579</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=579'>580</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=580'>581</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=581'>582</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=582'>583</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=583'>584</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=584'>585</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=585'>586</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=586'>587</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=587'>588</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=588'>589</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=589'>590</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=591'>592</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=592'>593</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:470\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=457'>458</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=458'>459</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=459'>460</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=466'>467</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=467'>468</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=468'>469</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=469'>470</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=470'>471</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=471'>472</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=472'>473</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=473'>474</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=474'>475</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=475'>476</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=476'>477</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=478'>479</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:410\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=390'>391</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=391'>392</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=392'>393</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=398'>399</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=399'>400</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=400'>401</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=401'>402</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=402'>403</a>\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=407'>408</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=408'>409</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=409'>410</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(self_outputs[\u001b[39m0\u001b[39;49m], hidden_states)\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=410'>411</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=411'>412</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:361\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=358'>359</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, input_tensor):\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=359'>360</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=360'>361</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(hidden_states)\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=361'>362</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n\u001b[1;32m    <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py?line=362'>363</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1105\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1104'>1105</a>\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward)\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///mnt/d/Code/UMBC_Capstone_2022/env/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m             \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = NewsDataset()\n",
    "val_dataset = NewsDataset(split='val')\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Model(device)\n",
    "\n",
    "train(train_dataset, val_dataset, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training binary classification model, try having a tanh activation function as output. The output would be tanh but use a function to transform to logits: \n",
    "\n",
    "https://stats.stackexchange.com/a/221905\n",
    "https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python/36440463#36440463"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8815d38e2f44888bf9d2d27291709bfd1a05b28bf990b1e1d952750508868489"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
